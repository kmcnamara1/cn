{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handlers could be found for logger \"sagemaker\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/tensorflow_core/__init__.py:1473: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import depdendencies\n",
    "import shutil\n",
    "import sagemaker\n",
    "import os\n",
    "import numpy as np\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.tensorflow import TensorFlowModel\n",
    "from sagemaker.tensorflow import TensorFlow\n",
    "from tensorflow.python.keras.preprocessing.image import load_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Example: S3 output upload\n",
    "# !aws s3 cp images/test s3://tickercardiology-echocv-sagemaker/model-data/test --recursive\n",
    "# !aws s3 cp images/train s3://tickercardiology-echocv-sagemaker/model-data/train --recursive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Example: test model on image\n",
    "test_image = load_img(\"images/test/a2c/a2c-1_00.jpg\", target_size=(224, 224))\n",
    "test_image_array = np.array(test_image).reshape((1, 224, 224, 3))\n",
    "result = predictor.predict({'inputs_input': test_image_array})\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "region = sagemaker_session.boto_session.region_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set S3 address to training data\n",
    "bucket = \"tickercardiology-echocv-sagemaker\"\n",
    "key = \"model-data\"\n",
    "training_data_uri = \"s3://{}/{}/train/\".format(bucket, key)\n",
    "# Set S3 address to validation data\n",
    "bucket = \"tickercardiology-echocv-sagemaker\"\n",
    "key = \"model-data\"\n",
    "validation_data_uri = \"s3://{}/{}/validation/\".format(bucket, key)\n",
    "# Set S3 address to test data\n",
    "bucket = \"tickercardiology-echocv-sagemaker\"\n",
    "key = \"model-data\"\n",
    "testing_data_uri = \"s3://{}/{}/test/\".format(bucket, key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mtf\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mnumpy\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnp\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mjson\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow.python.keras.models\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Sequential\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow.python.keras.layers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Dense, Conv2D, MaxPool2D , Flatten, Dropout\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow.python.keras.preprocessing.image\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m ImageDataGenerator\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow.keras.optimizers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Adam\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow.python.keras.losses\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m categorical_crossentropy  \n",
      "\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mnumpy\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnp\u001b[39;49;00m\n",
      "\n",
      "INPUT_TENSOR_NAME = \u001b[33m\"\u001b[39;49;00m\u001b[33minputs_input\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[37m# Watch out, it needs to match the name of the first layer + \"_input\"\u001b[39;49;00m\n",
      "BATCH_SIZE = \u001b[34m64\u001b[39;49;00m\n",
      "HEIGHT = \u001b[34m224\u001b[39;49;00m\n",
      "WIDTH = \u001b[34m224\u001b[39;49;00m\n",
      "DEPTH = \u001b[34m3\u001b[39;49;00m\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mmodel_compile\u001b[39;49;00m(learning_rate):\n",
      "    model = Sequential()\n",
      "    model.add(Conv2D(input_shape=(HEIGHT,WIDTH,DEPTH),filters=\u001b[34m64\u001b[39;49;00m,kernel_size=(\u001b[34m3\u001b[39;49;00m,\u001b[34m3\u001b[39;49;00m),padding=\u001b[33m\"\u001b[39;49;00m\u001b[33msame\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,activation=\u001b[33m\"\u001b[39;49;00m\u001b[33mrelu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,name=\u001b[33m\"\u001b[39;49;00m\u001b[33minputs\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m))\n",
      "    model.add(Conv2D(filters=\u001b[34m64\u001b[39;49;00m,kernel_size=(\u001b[34m3\u001b[39;49;00m,\u001b[34m3\u001b[39;49;00m),padding=\u001b[33m\"\u001b[39;49;00m\u001b[33msame\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, activation=\u001b[33m\"\u001b[39;49;00m\u001b[33mrelu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m))\n",
      "    model.add(MaxPool2D(pool_size=(\u001b[34m2\u001b[39;49;00m,\u001b[34m2\u001b[39;49;00m),strides=(\u001b[34m2\u001b[39;49;00m,\u001b[34m2\u001b[39;49;00m)))\n",
      "    model.add(Conv2D(filters=\u001b[34m128\u001b[39;49;00m, kernel_size=(\u001b[34m3\u001b[39;49;00m,\u001b[34m3\u001b[39;49;00m), padding=\u001b[33m\"\u001b[39;49;00m\u001b[33msame\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, activation=\u001b[33m\"\u001b[39;49;00m\u001b[33mrelu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m))\n",
      "    model.add(Conv2D(filters=\u001b[34m128\u001b[39;49;00m, kernel_size=(\u001b[34m3\u001b[39;49;00m,\u001b[34m3\u001b[39;49;00m), padding=\u001b[33m\"\u001b[39;49;00m\u001b[33msame\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, activation=\u001b[33m\"\u001b[39;49;00m\u001b[33mrelu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m))\n",
      "    model.add(MaxPool2D(pool_size=(\u001b[34m2\u001b[39;49;00m,\u001b[34m2\u001b[39;49;00m),strides=(\u001b[34m2\u001b[39;49;00m,\u001b[34m2\u001b[39;49;00m)))\n",
      "    model.add(Conv2D(filters=\u001b[34m256\u001b[39;49;00m, kernel_size=(\u001b[34m3\u001b[39;49;00m,\u001b[34m3\u001b[39;49;00m), padding=\u001b[33m\"\u001b[39;49;00m\u001b[33msame\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, activation=\u001b[33m\"\u001b[39;49;00m\u001b[33mrelu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m))\n",
      "    model.add(Conv2D(filters=\u001b[34m256\u001b[39;49;00m, kernel_size=(\u001b[34m3\u001b[39;49;00m,\u001b[34m3\u001b[39;49;00m), padding=\u001b[33m\"\u001b[39;49;00m\u001b[33msame\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, activation=\u001b[33m\"\u001b[39;49;00m\u001b[33mrelu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m))\n",
      "    model.add(Conv2D(filters=\u001b[34m256\u001b[39;49;00m, kernel_size=(\u001b[34m3\u001b[39;49;00m,\u001b[34m3\u001b[39;49;00m), padding=\u001b[33m\"\u001b[39;49;00m\u001b[33msame\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, activation=\u001b[33m\"\u001b[39;49;00m\u001b[33mrelu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m))\n",
      "    model.add(MaxPool2D(pool_size=(\u001b[34m2\u001b[39;49;00m,\u001b[34m2\u001b[39;49;00m),strides=(\u001b[34m2\u001b[39;49;00m,\u001b[34m2\u001b[39;49;00m)))\n",
      "    model.add(Conv2D(filters=\u001b[34m512\u001b[39;49;00m, kernel_size=(\u001b[34m3\u001b[39;49;00m,\u001b[34m3\u001b[39;49;00m), padding=\u001b[33m\"\u001b[39;49;00m\u001b[33msame\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, activation=\u001b[33m\"\u001b[39;49;00m\u001b[33mrelu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m))\n",
      "    model.add(Conv2D(filters=\u001b[34m512\u001b[39;49;00m, kernel_size=(\u001b[34m3\u001b[39;49;00m,\u001b[34m3\u001b[39;49;00m), padding=\u001b[33m\"\u001b[39;49;00m\u001b[33msame\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, activation=\u001b[33m\"\u001b[39;49;00m\u001b[33mrelu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m))\n",
      "    model.add(Conv2D(filters=\u001b[34m512\u001b[39;49;00m, kernel_size=(\u001b[34m3\u001b[39;49;00m,\u001b[34m3\u001b[39;49;00m), padding=\u001b[33m\"\u001b[39;49;00m\u001b[33msame\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, activation=\u001b[33m\"\u001b[39;49;00m\u001b[33mrelu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m))\n",
      "    model.add(MaxPool2D(pool_size=(\u001b[34m2\u001b[39;49;00m,\u001b[34m2\u001b[39;49;00m),strides=(\u001b[34m2\u001b[39;49;00m,\u001b[34m2\u001b[39;49;00m)))\n",
      "    model.add(Conv2D(filters=\u001b[34m512\u001b[39;49;00m, kernel_size=(\u001b[34m3\u001b[39;49;00m,\u001b[34m3\u001b[39;49;00m), padding=\u001b[33m\"\u001b[39;49;00m\u001b[33msame\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, activation=\u001b[33m\"\u001b[39;49;00m\u001b[33mrelu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m))\n",
      "    model.add(Conv2D(filters=\u001b[34m512\u001b[39;49;00m, kernel_size=(\u001b[34m3\u001b[39;49;00m,\u001b[34m3\u001b[39;49;00m), padding=\u001b[33m\"\u001b[39;49;00m\u001b[33msame\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, activation=\u001b[33m\"\u001b[39;49;00m\u001b[33mrelu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m))\n",
      "    model.add(Conv2D(filters=\u001b[34m512\u001b[39;49;00m, kernel_size=(\u001b[34m3\u001b[39;49;00m,\u001b[34m3\u001b[39;49;00m), padding=\u001b[33m\"\u001b[39;49;00m\u001b[33msame\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, activation=\u001b[33m\"\u001b[39;49;00m\u001b[33mrelu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m))\n",
      "    model.add(MaxPool2D(pool_size=(\u001b[34m2\u001b[39;49;00m,\u001b[34m2\u001b[39;49;00m),strides=(\u001b[34m2\u001b[39;49;00m,\u001b[34m2\u001b[39;49;00m)))\n",
      "    model.add(Dropout(\u001b[34m0.5\u001b[39;49;00m))\n",
      "    model.add(Flatten())\n",
      "    model.add(Dense(units=\u001b[34m4096\u001b[39;49;00m,activation=\u001b[33m\"\u001b[39;49;00m\u001b[33mrelu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m))\n",
      "    model.add(Dense(units=\u001b[34m4096\u001b[39;49;00m,activation=\u001b[33m\"\u001b[39;49;00m\u001b[33mrelu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m))\n",
      "    model.add(Dense(units=\u001b[34m8\u001b[39;49;00m, activation=\u001b[33m\"\u001b[39;49;00m\u001b[33msoftmax\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m))\n",
      "\n",
      "    opt = Adam(lr=learning_rate)\n",
      "    model.compile(optimizer=opt, loss=\u001b[33m'\u001b[39;49;00m\u001b[33mcategorical_crossentropy\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, metrics=[\u001b[33m'\u001b[39;49;00m\u001b[33maccuracy\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "\n",
      "    \u001b[34mreturn\u001b[39;49;00m model\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mmodel_fit\u001b[39;49;00m(model, train_data_dir, epoch, batch_size):\n",
      "    \n",
      "    train_datagen = ImageDataGenerator(rescale=\u001b[34m1.\u001b[39;49;00m/\u001b[34m255\u001b[39;49;00m, shear_range=\u001b[34m0.2\u001b[39;49;00m, zoom_range=\u001b[34m0.2\u001b[39;49;00m, horizontal_flip=\u001b[36mTrue\u001b[39;49;00m)\n",
      "    train_generator = train_datagen.flow_from_directory(train_data_dir, target_size=(HEIGHT, WIDTH), batch_size=batch_size, shuffle=\u001b[36mTrue\u001b[39;49;00m,)    \n",
      "    \n",
      "    history = model.fit(train_generator, epochs=epoch)\n",
      "\n",
      "    \u001b[34mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mHistory:\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    \u001b[34mprint\u001b[39;49;00m(history)\n",
      "\n",
      "    \u001b[34mreturn\u001b[39;49;00m model, history\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32m_load_training_data\u001b[39;49;00m(train_data_dir, batch_size):\n",
      "    \u001b[34massert\u001b[39;49;00m os.path.exists(train_data_dir), (\u001b[33m\"\u001b[39;49;00m\u001b[33mUnable to find images resources for input, are you sure you downloaded them ?\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\n",
      "    datagen = ImageDataGenerator(rescale=\u001b[34m1.\u001b[39;49;00m/\u001b[34m255\u001b[39;49;00m, shear_range=\u001b[34m0.2\u001b[39;49;00m, zoom_range=\u001b[34m0.2\u001b[39;49;00m, horizontal_flip=\u001b[36mTrue\u001b[39;49;00m)\n",
      "    generator = datagen.flow_from_directory(train_data_dir, target_size=(HEIGHT, WIDTH), batch_size=batch_size, shuffle=\u001b[36mTrue\u001b[39;49;00m,)\n",
      "    images, labels = generator.next()\n",
      "\n",
      "    \u001b[34mreturn\u001b[39;49;00m {INPUT_TENSOR_NAME: images}, labels\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32m_load_testing_data\u001b[39;49;00m(test_data_dir, batch_size):\n",
      "    \n",
      "    datagen = ImageDataGenerator(rescale=\u001b[34m1.\u001b[39;49;00m/\u001b[34m255\u001b[39;49;00m)\n",
      "    generator = datagen.flow_from_directory(test_data_dir, target_size=(HEIGHT, WIDTH), batch_size=batch_size, shuffle=\u001b[36mTrue\u001b[39;49;00m,)\n",
      "    images, labels = generator.next()\n",
      "\n",
      "    \u001b[34mreturn\u001b[39;49;00m {INPUT_TENSOR_NAME: images}, labels\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32m_parse_args\u001b[39;49;00m():\n",
      "    parser = argparse.ArgumentParser()\n",
      "\u001b[37m#     parser.add_argument('--model_dir', type=str, default=os.environ.get('SM_MODEL_DIR'))\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--model_dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--train\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ.get(\u001b[33m'\u001b[39;49;00m\u001b[33mSM_CHANNEL_TRAINING\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--batch_size\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m64\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--learning_rate\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m, default=\u001b[34m0.001\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--epochs\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m10\u001b[39;49;00m)\n",
      "    \n",
      "    \u001b[34mreturn\u001b[39;49;00m parser.parse_known_args()\n",
      "\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m\"\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\n",
      "    args, unknown = _parse_args()\n",
      "    raw_model = model_compile(args.learning_rate)\n",
      "    view_classifier, history = model_fit(raw_model, args.train, args.epochs, args.batch_size)\n",
      "    \n",
      "\u001b[37m#     if args.current_host == args.hosts[0]:\u001b[39;49;00m\n",
      "\u001b[37m#     view_classifier.save(args.model_dir)\u001b[39;49;00m\n",
      "\u001b[37m#     view_classifier.save(os.path.join(args.model_dir, '000000001'), 'my_model.h5')\u001b[39;49;00m\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "# Display training script\n",
    "!pygmentize 'keras_model_fn.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TensorFlow2 estimator \n",
    "# bucket = \"sagemaker-ap-southeast-2-611188727347\"\n",
    "# key = \"test1\"\n",
    "# model_path = \"s3://{}/{}/\".format(bucket, key)\n",
    "\n",
    "estimator2 = TensorFlow(entry_point='keras_model_fn.py',\n",
    "                             role=role,\n",
    "                             train_instance_count=1,\n",
    "#                              train_instance_type='ml.m5.2xlarge', #$0.672\n",
    "                             train_instance_type='ml.g4dn.xlarge', #$0.958\n",
    "                             framework_version='2.1.0',\n",
    "                             py_version='py2',\n",
    "                             hyperparameters={'epochs': 1},\n",
    "#                              model_dir = model_path,\n",
    "                             distributions={'parameter_server': {'enabled': True}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-11-05 05:19:58 Starting - Starting the training job...\n",
      "2020-11-05 05:20:00 Starting - Launching requested ML instances.."
     ]
    }
   ],
   "source": [
    "# Start training model\n",
    "\n",
    "estimator2.fit(training_data_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ClientError",
     "evalue": "An error occurred (ValidationException) when calling the CreateModel operation: Could not find model data at s3://sagemaker-ap-southeast-2-611188727347/tensorflow-training-2020-11-05-04-56-30-245/output/model.tar.gz.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mClientError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-36d1893ad6d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredictor2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mestimator2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeploy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial_instance_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstance_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ml.m5.2xlarge'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/ec2-user/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/sagemaker/estimator.pyc\u001b[0m in \u001b[0;36mdeploy\u001b[0;34m(self, initial_instance_count, instance_type, accelerator_type, endpoint_name, use_compiled_model, update_endpoint, wait, model_name, kms_key, data_capture_config, tags, **kwargs)\u001b[0m\n\u001b[1;32m    743\u001b[0m             \u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    744\u001b[0m             \u001b[0mkms_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkms_key\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 745\u001b[0;31m             \u001b[0mdata_capture_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_capture_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    746\u001b[0m         )\n\u001b[1;32m    747\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ec2-user/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/sagemaker/tensorflow/serving.pyc\u001b[0m in \u001b[0;36mdeploy\u001b[0;34m(self, initial_instance_count, instance_type, accelerator_type, endpoint_name, update_endpoint, tags, kms_key, wait, data_capture_config)\u001b[0m\n\u001b[1;32m    211\u001b[0m             \u001b[0mkms_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkms_key\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m             \u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m             \u001b[0mdata_capture_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_capture_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m         )\n\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ec2-user/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/sagemaker/model.pyc\u001b[0m in \u001b[0;36mdeploy\u001b[0;34m(self, initial_instance_count, instance_type, accelerator_type, endpoint_name, update_endpoint, tags, kms_key, wait, data_capture_config)\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"{}{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_prefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompiled_model_suffix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_sagemaker_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccelerator_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    546\u001b[0m         production_variant = sagemaker.production_variant(\n\u001b[1;32m    547\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstance_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_instance_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccelerator_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccelerator_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ec2-user/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/sagemaker/model.pyc\u001b[0m in \u001b[0;36m_create_sagemaker_model\u001b[0;34m(self, instance_type, accelerator_type, tags)\u001b[0m\n\u001b[1;32m    197\u001b[0m             \u001b[0mvpc_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvpc_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m             \u001b[0menable_network_isolation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menable_network_isolation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m             \u001b[0mtags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m         )\n\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ec2-user/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/sagemaker/session.pyc\u001b[0m in \u001b[0;36mcreate_model\u001b[0;34m(self, name, role, container_defs, vpc_config, enable_network_isolation, primary_container, tags)\u001b[0m\n\u001b[1;32m   2144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2145\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2146\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mcreate_model_request\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2147\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mClientError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2148\u001b[0m             \u001b[0merror_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Error\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Code\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ec2-user/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/botocore/client.pyc\u001b[0m in \u001b[0;36m_api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m                     \"%s() only accepts keyword arguments.\" % py_operation_name)\n\u001b[1;32m    356\u001b[0m             \u001b[0;31m# The \"self\" in this scope is referring to the BaseClient.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_api_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m         \u001b[0m_api_call\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_operation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ec2-user/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/botocore/client.pyc\u001b[0m in \u001b[0;36m_make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    674\u001b[0m             \u001b[0merror_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Error\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Code\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m             \u001b[0merror_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_response\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mClientError\u001b[0m: An error occurred (ValidationException) when calling the CreateModel operation: Could not find model data at s3://sagemaker-ap-southeast-2-611188727347/tensorflow-training-2020-11-05-04-56-30-245/output/model.tar.gz."
     ]
    }
   ],
   "source": [
    "predictor2 = estimator2.deploy(initial_instance_count=1, instance_type='ml.m5.2xlarge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker.Session().delete_endpoint(predictor2.endpoint)"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "conda_tensorflow_p27",
   "language": "python",
   "name": "conda_tensorflow_p27"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
