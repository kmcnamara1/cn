{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "This Notebook has cells for training, validating and deploying a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import depdendencies\n",
    "kernel = conda_tensorflow2_p36"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import sagemaker\n",
    "import os\n",
    "import numpy as np\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.tensorflow import TensorFlowModel\n",
    "from sagemaker.tensorflow import TensorFlow\n",
    "from tensorflow.python.keras.preprocessing.image import load_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "region = sagemaker_session.boto_session.region_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/ml/model\n"
     ]
    }
   ],
   "source": [
    "os.environ['SM_MODEL_DIR'] = \"/opt/ml/model\"\n",
    "print(os.environ['SM_MODEL_DIR'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S3 bucket URI for training, validation and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set S3 address to training data\n",
    "bucket = \"tickercardiology-echocv-sagemaker\"\n",
    "key = \"model-data\"\n",
    "training_data_uri = \"s3://{}/{}/train/\".format(bucket, key)\n",
    "# Set S3 address to validation data\n",
    "validation_data_uri = \"s3://{}/{}/test/\".format(bucket, key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display training script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pygmentize 'keras_model_fn.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create TensorFlow2 estimator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator2 = TensorFlow(entry_point='keras_model_fn.py',\n",
    "                        role=role,\n",
    "                        train_instance_count=1,\n",
    "                        train_instance_type='ml.g4dn.xlarge', #$0.958\n",
    "                        framework_version='2.1.0',\n",
    "                        py_version='py36',\n",
    "                        hyperparameters={\n",
    "                                        'epochs': 15,\n",
    "                                        'batch_size': 64,\n",
    "                                        'learning_rate': 1e-5},\n",
    "                        script_mode=True,\n",
    "                        model_dir = os.environ['SM_MODEL_DIR'],\n",
    "                        distributions={'parameter_server': {'enabled': True}})\n",
    "\n",
    "bucket = \"sagemaker-ap-southeast-2-611188727347\"\n",
    "key = \"test1\"\n",
    "model_path = \"s3://{}/{}/\".format(bucket, key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start training model\n",
    "model save path:\n",
    "S3://{sagemaker-ap-southeast-2-611188727347}.{tensorflow-training-year-month-data-hour-minute-second-ms}/output/model.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator2.fit({'training': training_data_uri, 'validation': validation_data_uri})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model from S3, if estimator trained in this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor2 = estimator2.deploy(initial_instance_count=1, instance_type='ml.m5.2xlarge', endpoint_name='epoch15-3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OR\n",
    "## Load model from S3, if estimator not trained in this notebook "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = \"sagemaker-ap-southeast-2-611188727347\"\n",
    "key = \"tensorflow-training-2020-11-09-01-37-28-348\"\n",
    "model_path = \"s3://{}/{}/output/model.tar.gz\".format(bucket, key)\n",
    "\n",
    "model = TensorFlowModel(model_data=model_path, \n",
    "                        role=role,\n",
    "                        framework_version='2.1.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = model.deploy(initial_instance_count=1, instance_type='ml.m5.2xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete the endpoints to save resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker.Session().delete_endpoint(predictor.endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(directory, predictor):\n",
    "    \"\"\"\n",
    "    Classifies echo images in given directory\n",
    "\n",
    "    @param directory: folder with jpg echo images for classification\n",
    "    \"\"\"\n",
    "    imagedict = {}\n",
    "    predictions = {}\n",
    "    for filename in os.listdir(directory):\n",
    "        if \"jpg\" in filename:\n",
    "            image = load_img(directory + filename, target_size=(224, 224))\n",
    "            imagedict[filename] = np.array(image).reshape((1, 224, 224, 3))\n",
    "        \n",
    "    for filename in imagedict:\n",
    "        result = predictor.predict({'inputs_input': imagedict[filename]})\n",
    "        predictions[filename] = result[\"predictions\"][0]\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = classify(\"images/test/random/\", predictor2)\n",
    "print(preds)\n",
    "\n",
    "for p in preds:\n",
    "    print(preds[p])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Download the .pb and variables from S3\n",
    "# !aws s3 cp s3://sagemaker-ap-southeast-2-611188727347/tensorflow-training-2020-11-06-03-49-07-502/model/1 temp/1 --recursive \n",
    "## Run this in terminal:    tar -C \"$PWD\" -czf model.tar.gz temp/\n",
    "## Copy the tar.gz to the s3://kate//tensorflow-training-date/output/\n",
    "# !aws s3 cp model.tar.gz s3://sagemaker-ap-southeast-2-611188727347/tensorflow-training-2020-11-06-03-49-07-502/output/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "conda_tensorflow2_p36",
   "language": "python",
   "name": "conda_tensorflow2_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
